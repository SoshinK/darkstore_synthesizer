{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3kdi7DbKWns"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_d3BsUjkKSuK",
    "outputId": "438385e4-5d52-4e27-bf41-f7c29099cf7a"
   },
   "outputs": [],
   "source": [
    "# setup vulkan\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json\n",
    "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
    "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
    "!apt-get install -y --no-install-recommends libvulkan-dev\n",
    "# dependencies\n",
    "!pip install --upgrade mani_skill tyro\n",
    "!pip install sapien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1qtqv4EKaCf"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import site\n",
    "    site.main() # run this so local pip installs are recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY_gz9g8LNMH"
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dto7MnOmMRka",
    "outputId": "c7997e6b-cbc4-4c6d-a57c-6165cc528a5b"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import gymnasium as gym\n",
    "import mani_skill.envs\n",
    "import torch\n",
    "import time\n",
    "num_envs = 512 # you can go up higher on better GPUs, this is mostly memory constrained\n",
    "env = gym.make(\"PickCube-v1\", num_envs=num_envs, obs_mode=\"rgbd\")\n",
    "env.unwrapped.print_sim_details()\n",
    "obs, _ = env.reset(seed=0)\n",
    "done = False\n",
    "start_time = time.time()\n",
    "total_rew = 0\n",
    "while not done:\n",
    "    # note that env.action_space is now a batched action space\n",
    "    obs, rew, terminated, truncated, info = env.step(torch.from_numpy(env.action_space.sample()))\n",
    "    done = (terminated | truncated).any() # stop if any environment terminates/truncates\n",
    "N = num_envs * info[\"elapsed_steps\"][0].item()\n",
    "dt = time.time() - start_time\n",
    "FPS = N / (dt)\n",
    "print(f\"Frames Per Second = {N} / {dt} = {FPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "8eVbxMpLMWXB",
    "outputId": "8de621a4-aa7e-4a39-e2bc-511ecb244677"
   },
   "outputs": [],
   "source": [
    "# visualize the image data from the environment and inspect the data\n",
    "print(obs.keys())\n",
    "print(obs['sensor_data'].keys())\n",
    "print(obs['sensor_data']['base_camera'].keys())\n",
    "print(obs['sensor_data']['base_camera']['rgb'].shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs['sensor_data']['base_camera']['rgb'][0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5Z7A6OqKdzB"
   },
   "source": [
    "### Shit class\n",
    "Right here you need to change MyEnv to smth other if updating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "JBgtCoQjKlOh",
    "outputId": "ec10628d-a5be-4531-b066-c216f10c08d4"
   },
   "outputs": [],
   "source": [
    "from mani_skill.envs.tasks.empty_env import EmptyEnv\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from mani_skill.agents.robots.fetch import FETCH_WHEELS_COLLISION_BIT\n",
    "from mani_skill.utils.building.ground import build_ground\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils import common, sapien_utils\n",
    "import sapien\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import mani_skill.envs\n",
    "from tqdm import tqdm\n",
    "from mani_skill.utils.wrappers import RecordEpisode\n",
    "from transforms3d import quaternions\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "\n",
    "\n",
    "ENV_NAME = generate_random_string()\n",
    "\n",
    "@register_env(ENV_NAME, max_episode_steps=200000)\n",
    "class OurEnv(BaseEnv):\n",
    "    SUPPORTED_REWARD_MODES = [\"none\"]\n",
    "    \"\"\"\n",
    "    This is just a very smart environment for goida transformation from ss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", **kwargs):\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at([1.25, -1.25, 1.5], [0.0, 0.0, 0.2])\n",
    "        return [CameraConfig(\"base_camera\", pose, 256, 256, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([3.5, 3.5, 1], [0.0, 0.0, 0.35])\n",
    "        return CameraConfig(\n",
    "            \"render_camera\", pose=pose, width=512, height=512, fov=1, near=0.01, far=100\n",
    "        )\n",
    "\n",
    "    def _load_agent(self, options: dict):\n",
    "        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.ground = build_ground(self.scene)\n",
    "        self.ground.set_collision_group_bit(group=2, bit_idx=30, bit=1)\n",
    "        self._load_scene_from_json(options)\n",
    "\n",
    "\n",
    "    def _process_string(self, s):\n",
    "        if '_' in s:\n",
    "            return s.split('_',1)[0] + '.obj'\n",
    "        if '.' in s:\n",
    "            return s.split('.',1)[0] + '.obj'\n",
    "        return s + '.obj'\n",
    "\n",
    "    def _temp_process_string(self, s):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in \"_.\" or char.isdigit():\n",
    "                return s[:i] + \".obj\"\n",
    "        return s + \".obj\"\n",
    "\n",
    "    def _get_absolute_matrix(self, node, nodes_dict):\n",
    "        current_matrix = np.array(node[2][\"matrix\"])\n",
    "        parent_name = node[0]\n",
    "        while parent_name != \"world\":\n",
    "            # print(f\"Doing GOIDA IN PROCESS for name {node[1]} with parent {node[0]}\")\n",
    "            parent_node = nodes_dict[parent_name]\n",
    "            parent_matrix = np.array(parent_node[2][\"matrix\"])\n",
    "            current_matrix = parent_matrix @ current_matrix\n",
    "            parent_name = parent_node[0]\n",
    "        return current_matrix\n",
    "\n",
    "    def _get_pq(self, matrix, origin):\n",
    "        matrix = np.array(matrix)\n",
    "        q = quaternions.mat2quat(matrix[:3,:3])\n",
    "        p = matrix[:-1, 3] - origin\n",
    "        return p, q\n",
    "\n",
    "    def _load_scene_from_json(self, options: dict):\n",
    "        super()._load_scene(options)\n",
    "        self.actors = []\n",
    "\n",
    "        assets_dir = options.get(\"assets_dir\", \"./assets/\")\n",
    "        scale = np.array(options.get(\"scale\", [1.0, 1.0, 1.0]))\n",
    "        origin = np.array(options.get(\"origin\", [0.0, 1.0, 0.0]))\n",
    "\n",
    "        with open('customize.json', \"r\") as f: # big_scene , one_shelf_many_milk_scene , customize\n",
    "            data = json.load(f)\n",
    "\n",
    "        nodes_dict = {}\n",
    "        for node in data[\"graph\"]:\n",
    "            nodes_dict[node[1]] = node\n",
    "\n",
    "        for node in data[\"graph\"]:\n",
    "            parent_name, obj_name, props = node\n",
    "            if ('/' not in obj_name):\n",
    "                abs_matrix = self._get_absolute_matrix(node, nodes_dict)\n",
    "\n",
    "                p, q = self._get_pq(abs_matrix, origin)\n",
    "\n",
    "                asset_file = os.path.join(assets_dir, self._temp_process_string(obj_name))\n",
    "                if not os.path.exists(asset_file):\n",
    "                    asset_file = os.path.splitext(asset_file)[0] + \".glb\"\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    print(\"Not found file for \" + obj_name + \" =(\" + \" ( \" + asset_file + \" )\")\n",
    "                else:\n",
    "                    # print(\"Found file for \" + obj_name + \" =)\" + \" ( \" + asset_file + \" )\")\n",
    "                    builder = self.scene.create_actor_builder()\n",
    "                    builder.add_visual_from_file(filename=asset_file, scale=scale)\n",
    "                    builder.set_initial_pose(sapien.Pose(p=p, q=q))\n",
    "\n",
    "                    if obj_name.startswith('shelf'):\n",
    "                        builder.add_nonconvex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                        actor = builder.build_static(name=obj_name)\n",
    "                    else:\n",
    "                        builder.add_convex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                        actor = builder.build(name=obj_name)\n",
    "\n",
    "                    self.actors.append(actor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        if self.robot_uids == \"fetch\":\n",
    "            qpos = np.array(\n",
    "                [\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0.386,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    -np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 3,\n",
    "                    0,\n",
    "                    0.015,\n",
    "                    0.015,\n",
    "                ]\n",
    "            )\n",
    "            self.agent.reset(qpos)\n",
    "            self.agent.robot.set_pose(sapien.Pose([10.0, 10, 0.0]))\n",
    "\n",
    "            self.ground.set_collision_group_bit(\n",
    "                group=2, bit_idx=FETCH_WHEELS_COLLISION_BIT, bit=1\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        return dict()\n",
    "\n",
    "\n",
    "env = gym.make(ENV_NAME, robot_uids='fetch', num_envs=1, render_mode=\"rgb_array\", enable_shadow=True)\n",
    "\n",
    "env = RecordEpisode(\n",
    "    env,\n",
    "    \"./videos\", # the directory to save replay videos and trajectories to\n",
    "    # on GPU sim we record intervals, not by single episodes as there are multiple envs\n",
    "    # each 100 steps a new video is saved\n",
    "    max_steps_per_video=100\n",
    ")\n",
    "\n",
    "# step through the environment with random actions\n",
    "obs, _ = env.reset()\n",
    "\n",
    "\n",
    "viewer = env.render()\n",
    "if isinstance(viewer, sapien.utils.Viewer):\n",
    "    viewer.paused = False\n",
    "env.render()\n",
    "\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(torch.zeros_like(torch.from_numpy(action)))\n",
    "\n",
    "    env.render()\n",
    "    # env.render_human() # will render with a window if possible\n",
    "env.close()\n",
    "from IPython.display import Video\n",
    "Video(\"./videos/0.mp4\", embed=True, width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk_eXp_rlH0O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
