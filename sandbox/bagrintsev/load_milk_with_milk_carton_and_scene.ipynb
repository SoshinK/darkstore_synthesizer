{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3kdi7DbKWns"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_d3BsUjkKSuK",
    "outputId": "a9583aed-1aad-4535-c52d-e85059ebb369"
   },
   "outputs": [],
   "source": [
    "# setup vulkan\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json\n",
    "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
    "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
    "!apt-get install -y --no-install-recommends libvulkan-dev\n",
    "# dependencies\n",
    "!pip install --upgrade mani_skill tyro\n",
    "!pip install sapien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1qtqv4EKaCf"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import site\n",
    "    site.main() # run this so local pip installs are recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY_gz9g8LNMH"
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dto7MnOmMRka",
    "outputId": "e693c774-73f6-4806-9a53-2433d6d6d908"
   },
   "outputs": [],
   "source": [
    "import mani_skill.envs\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "num_envs = 512 # you can go up higher on better GPUs, this is mostly memory constrained\n",
    "env = gym.make(\"PickCube-v1\", num_envs=num_envs, obs_mode=\"rgbd\")\n",
    "env.unwrapped.print_sim_details()\n",
    "obs, _ = env.reset(seed=0)\n",
    "done = False\n",
    "start_time = time.time()\n",
    "total_rew = 0\n",
    "while not done:\n",
    "    # note that env.action_space is now a batched action space\n",
    "    obs, rew, terminated, truncated, info = env.step(torch.from_numpy(env.action_space.sample()))\n",
    "    done = (terminated | truncated).any() # stop if any environment terminates/truncates\n",
    "N = num_envs * info[\"elapsed_steps\"][0].item()\n",
    "dt = time.time() - start_time\n",
    "FPS = N / (dt)\n",
    "print(f\"Frames Per Second = {N} / {dt} = {FPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "8eVbxMpLMWXB",
    "outputId": "dd115fd4-f23a-4c48-d897-876249684d5b"
   },
   "outputs": [],
   "source": [
    "# visualize the image data from the environment and inspect the data\n",
    "print(obs.keys())\n",
    "print(obs['sensor_data'].keys())\n",
    "print(obs['sensor_data']['base_camera'].keys())\n",
    "print(obs['sensor_data']['base_camera']['rgb'].shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs['sensor_data']['base_camera']['rgb'][0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5Z7A6OqKdzB"
   },
   "source": [
    "### Shit class\n",
    "Right here you need to change MyEnv to smth other if updating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "JBgtCoQjKlOh",
    "outputId": "31832be6-cb84-455e-8005-584e80cc1d0c"
   },
   "outputs": [],
   "source": [
    "from mani_skill.envs.tasks.empty_env import EmptyEnv\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from mani_skill.agents.robots.fetch import FETCH_WHEELS_COLLISION_BIT\n",
    "from mani_skill.utils.building.ground import build_ground\n",
    "# from mani_skill.utils.scene_builder import SceneBuilder\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils import common, sapien_utils\n",
    "import sapien\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import mani_skill.envs\n",
    "from tqdm import tqdm\n",
    "from mani_skill.utils.wrappers import RecordEpisode\n",
    "from transforms3d import quaternions\n",
    "\n",
    "\n",
    "ENV_NAME =\"MyEnv13\"\n",
    "\n",
    "@register_env(ENV_NAME, max_episode_steps=200000)\n",
    "class MyEmptyEnv(BaseEnv):\n",
    "    SUPPORTED_REWARD_MODES = [\"none\"]\n",
    "    \"\"\"\n",
    "    This is just a dummy environment for showcasing robots in a empty scene\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", **kwargs):\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at([1.25, -1.25, 1.5], [0.0, 0.0, 0.2])\n",
    "        return [CameraConfig(\"base_camera\", pose, 128, 128, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([1.25, -1.25, 1.5], [0.0, 0.0, 0.2])\n",
    "        return CameraConfig(\"render_camera\", pose, 2048, 2048, 1, 0.01, 100)\n",
    "\n",
    "    def _load_agent(self, options: dict):\n",
    "        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.ground = build_ground(self.scene)\n",
    "        self.ground.set_collision_group_bit(group=2, bit_idx=30, bit=1)\n",
    "        self._load_shelf_arrangement_from_json(options)\n",
    "\n",
    "\n",
    "        #===============================\n",
    "        # Load URDF articuation\n",
    "        #===============================\n",
    "        # loader = self.scene.create_urdf_loader()\n",
    "        # articulation_builders = loader.parse(str('./scene.urdf'))[\"articulation_builders\"]\n",
    "        # builder = articulation_builders[0]\n",
    "\n",
    "        # builder.initial_pose = sapien.Pose(p=[0, 0, 0.5])\n",
    "        # builder.build(name=\"my_articulation\")\n",
    "\n",
    "\n",
    "        #===============================\n",
    "        # Load actor from glb\n",
    "        #===============================\n",
    "        # scale=np.array([1.0, 1.0, 1.0])\n",
    "        # builder = self.scene.create_actor_builder()\n",
    "        # builder.add_convex_collision_from_file(\n",
    "        #     filename=\"/home/kvsoshin/Work/AIRI/ManiSkill/textures/milk_carton.glb\",\n",
    "        #     scale=scale\n",
    "        # )\n",
    "        # builder.add_visual_from_file(filename=\"/home/kvsoshin/Work/AIRI/ManiSkill/textures/milk_carton.glb\", scale=scale)\n",
    "        # builder.set_initial_pose(sapien.Pose(p=[0.0, 0.0, 0.0]))\n",
    "        # self.mesh = builder.build_static(name=\"mesh\")\n",
    "\n",
    "        #===============================\n",
    "        # Load JSON\n",
    "        #===============================\n",
    "\n",
    "\n",
    "        # self.meshes = []\n",
    "\n",
    "        # with open('./scene.json', 'r') as f:\n",
    "        #     d = json.load(f)\n",
    "        # d_processed = [d['graph'][i] for i in range(len(d['graph'])) if not 'geometry' in d['graph'][i][2]]\n",
    "        # d_processed = [obj for obj in d_processed if 'can' in obj[1]]\n",
    "        # hmatrices = [np.array(obj[2]['matrix']) for obj in d_processed]\n",
    "\n",
    "        # shift = np.array([0.1, 0.2, 0.3])\n",
    "        # transform = np.array([\n",
    "        #     [1, 0, 0, 0],\n",
    "        #     [0, 0, 1, 0],\n",
    "        #     [0, 1, 0, 0],\n",
    "        #     [0, 0, 0, 1]\n",
    "        # ])\n",
    "        # for i, matrix in enumerate(hmatrices):\n",
    "        #     # matrix = matrix @ transform\n",
    "        #     q = quaternions.mat2quat(matrix[:3,:3])\n",
    "        #     print(\"GOIDA\")\n",
    "\n",
    "        #     p = matrix[:-1, 3] # - shift\n",
    "\n",
    "        #     builder = self.scene.create_actor_builder()\n",
    "\n",
    "        #     scale = (1, 1, 1)\n",
    "        #     builder.add_convex_collision_from_file(\n",
    "        #         filename=\"milk_carton.glb\",\n",
    "        #         # scale=scale\n",
    "        #     )\n",
    "        #     builder.add_visual_from_file(filename=\"milk_carton.glb\", scale=scale)\n",
    "        #     builder.set_initial_pose(sapien.Pose(p=p, q=q))\n",
    "        #     self.meshes.append(builder.build(name=f\"mesh_{i}\"))\n",
    "\n",
    "\n",
    "\n",
    "    def _get_pq(self, matrix, origin):\n",
    "        matrix = np.array(matrix)\n",
    "        q = quaternions.mat2quat(matrix[:3,:3])\n",
    "        p = matrix[:-1, 3] - origin\n",
    "        return p, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _load_shelf_arrangement_from_json(self, options: dict):\n",
    "        super()._load_scene(options)\n",
    "\n",
    "        self.things = []\n",
    "\n",
    "        with open('./scene.json', 'r') as f:\n",
    "            d = json.load(f)\n",
    "\n",
    "        d_processed = [d['graph'][i] for i in range(len(d['graph'])) if not 'geometry' in d['graph'][i][2]]\n",
    "        things = [obj for obj in d_processed if 'milk' in obj[1]]\n",
    "        shelf = [obj for obj in d_processed if 'shelf' in obj[1]]\n",
    "\n",
    "        scale = np.array([1.0, 1.0, 1.0])\n",
    "        origin = np.array([0.0, 1.0, -0.0])\n",
    "\n",
    "        # make shelf:\n",
    "        if len(shelf) > 0:\n",
    "          p_shelf, q_shelf = self._get_pq(shelf[2]['matrix'], origin)\n",
    "          builder = self.scene.create_actor_builder()\n",
    "          builder.add_nonconvex_collision_from_file(\n",
    "              filename=\"./milk_carton.glb\",\n",
    "              scale=scale\n",
    "          )\n",
    "          builder.add_visual_from_file(filename=\"./milk_carton.glb\", scale=scale)\n",
    "          builder.set_initial_pose(sapien.Pose(p=p_shelf, q=q_shelf))\n",
    "          self.mesh = builder.build_static(name=\"shelf\")\n",
    "\n",
    "\n",
    "        # place things:\n",
    "        for i, thing_placement in enumerate(things):\n",
    "            p_thing, q_thing = self._get_pq(thing_placement[2]['matrix'], origin)\n",
    "\n",
    "            if 'milk_' in thing_placement[1]:\n",
    "                vis_name = './milk_carton.glb'\n",
    "                collision_fname = vis_name\n",
    "                shift_z= np.array([0., 0., 0.0285])\n",
    "            elif 'milk2' in thing_placement[1]:\n",
    "                vis_name = './milk_carton.glb'\n",
    "                collision_fname = vis_name\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            builder = self.scene.create_actor_builder()\n",
    "            builder.add_convex_collision_from_file(\n",
    "                filename=collision_fname,\n",
    "                scale=scale\n",
    "            )\n",
    "            builder.add_visual_from_file(filename=vis_name, scale=scale)\n",
    "            builder.set_initial_pose(sapien.Pose(p=p_thing + shift_z, q=q_thing))\n",
    "            self.things.append(builder.build(name=f\"{i}_{thing_placement[1]}\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _load_scene_from_json(self, options: dict):\n",
    "        super()._load_scene(options)\n",
    "        self.actors = []\n",
    "\n",
    "        assets_dir   = options.get(\"assets_dir\", \"./assets\")\n",
    "        scale        = np.array(options.get(\"scale\", [1.0, 1.0, 1.0]))\n",
    "        origin       = np.array(options.get(\"origin\", [0.0, 1.0, 0.0]))\n",
    "\n",
    "        with open('shit.json', \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for node in data[\"graph\"]:\n",
    "            parent_name, obj_name, props = node\n",
    "            if (obj_name == \"table\"):\n",
    "                p, q = self._get_pq(props[\"matrix\"], origin)\n",
    "\n",
    "                asset_file = os.path.join(assets_dir, obj_name)\n",
    "\n",
    "                builder = self.scene.create_actor_builder()\n",
    "                builder.add_convex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                builder.add_visual_from_file(filename=asset_file, scale=scale)\n",
    "                builder.set_initial_pose(sapien.Pose(p=p, q=q))\n",
    "\n",
    "                actor = builder.build(name=obj_name)\n",
    "                self.actors.append(actor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        if self.robot_uids == \"fetch\":\n",
    "            qpos = np.array(\n",
    "                [\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0.386,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    -np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 3,\n",
    "                    0,\n",
    "                    0.015,\n",
    "                    0.015,\n",
    "                ]\n",
    "            )\n",
    "            self.agent.reset(qpos)\n",
    "            self.agent.robot.set_pose(sapien.Pose([10.0, 10, 0.0]))\n",
    "\n",
    "            self.ground.set_collision_group_bit(\n",
    "                group=2, bit_idx=FETCH_WHEELS_COLLISION_BIT, bit=1\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        return dict()\n",
    "\n",
    "\n",
    "env = gym.make(ENV_NAME, robot_uids='fetch', num_envs=1, render_mode=\"rgb_array\", enable_shadow=True)\n",
    "\n",
    "pose = sapien_utils.look_at([3.25, -3.25, 1.5], [0.0, 0.0, 0.2])\n",
    "env._default_human_render_camera_configs = CameraConfig(\"render_camera\", pose, 2048, 2048, 1, 0.01, 100)\n",
    "\n",
    "env = RecordEpisode(\n",
    "    env,\n",
    "    \"./videos\", # the directory to save replay videos and trajectories to\n",
    "    # on GPU sim we record intervals, not by single episodes as there are multiple envs\n",
    "    # each 100 steps a new video is saved\n",
    "    max_steps_per_video=100\n",
    ")\n",
    "\n",
    "# step through the environment with random actions\n",
    "obs, _ = env.reset()\n",
    "\n",
    "\n",
    "viewer = env.render()\n",
    "if isinstance(viewer, sapien.utils.Viewer):\n",
    "    viewer.paused = False\n",
    "env.render()\n",
    "\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(torch.zeros_like(torch.from_numpy(action)))\n",
    "\n",
    "    env.render()\n",
    "    # env.render_human() # will render with a window if possible\n",
    "env.close()\n",
    "from IPython.display import Video\n",
    "Video(\"./videos/0.mp4\", embed=True, width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyMwdETlngpD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
