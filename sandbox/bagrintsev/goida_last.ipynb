{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3kdi7DbKWns"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_d3BsUjkKSuK",
    "outputId": "99f24181-1ec1-4539-d5ea-acdf78fe608b"
   },
   "outputs": [],
   "source": [
    "# setup vulkan\n",
    "!apt-get install -y --no-install-recommends libvulkan-dev\n",
    "# dependencies\n",
    "!pip install --upgrade mani_skill tyro\n",
    "!pip install sapien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1qtqv4EKaCf"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import site\n",
    "    site.main() # run this so local pip installs are recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY_gz9g8LNMH"
   },
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dto7MnOmMRka",
    "outputId": "c36dd539-ad76-4198-8534-aeaaeb43b67c"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import gymnasium as gym\n",
    "import mani_skill.envs\n",
    "import torch\n",
    "import time\n",
    "num_envs = 512 # you can go up higher on better GPUs, this is mostly memory constrained\n",
    "env = gym.make(\"PickCube-v1\", num_envs=num_envs, obs_mode=\"rgbd\")\n",
    "env.unwrapped.print_sim_details()\n",
    "obs, _ = env.reset(seed=0)\n",
    "done = False\n",
    "start_time = time.time()\n",
    "total_rew = 0\n",
    "while not done:\n",
    "    # note that env.action_space is now a batched action space\n",
    "    obs, rew, terminated, truncated, info = env.step(torch.from_numpy(env.action_space.sample()))\n",
    "    done = (terminated | truncated).any() # stop if any environment terminates/truncates\n",
    "N = num_envs * info[\"elapsed_steps\"][0].item()\n",
    "dt = time.time() - start_time\n",
    "FPS = N / (dt)\n",
    "print(f\"Frames Per Second = {N} / {dt} = {FPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "8eVbxMpLMWXB",
    "outputId": "7e40874b-5a0b-4f48-e15d-e3d031f79029"
   },
   "outputs": [],
   "source": [
    "# visualize the image data from the environment and inspect the data\n",
    "print(obs.keys())\n",
    "print(obs['sensor_data'].keys())\n",
    "print(obs['sensor_data']['base_camera'].keys())\n",
    "print(obs['sensor_data']['base_camera']['rgb'].shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs['sensor_data']['base_camera']['rgb'][0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5Z7A6OqKdzB"
   },
   "source": [
    "### Shit class\n",
    "Right here you need to change MyEnv to smth other if updating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JBgtCoQjKlOh",
    "outputId": "fc448fb2-1b16-4256-cef6-1e603dfc195e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 89.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from mani_skill.envs.tasks.empty_env import EmptyEnv\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from mani_skill.agents.robots.fetch import FETCH_WHEELS_COLLISION_BIT\n",
    "from mani_skill.utils.building.ground import build_ground\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils import common, sapien_utils\n",
    "import sapien\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import mani_skill.envs\n",
    "from tqdm import tqdm\n",
    "from mani_skill.utils.wrappers import RecordEpisode\n",
    "from transforms3d import quaternions\n",
    "import random\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description=\"Запуск сцены: <путь_к_JSON_файлу_сцены> <путь_к_assets> [--mapping_file <путь_к_JSON_файлу_c_названиями_текстур>]\"\n",
    "# )\n",
    "# parser.add_argument(\"json_file\", help=\"Путь к JSON файлу сцены\")\n",
    "# parser.add_argument(\"assets_dir\", help=\"Путь к директории с ассетами\")\n",
    "# parser.add_argument(\"--mapping_file\", help=\"Путь к JSON файлу с сопоставлением obj_name и конкретных asset_file\", default=None)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# json_file_path = args.json_file\n",
    "# assets_dir = args.assets_dir\n",
    "# mapping_file = args.mapping_file\n",
    "\n",
    "json_file_path = \"./2_shelf_2_milk.json\"\n",
    "assets_dir = './assets/'\n",
    "mapping_file = './connect.json'\n",
    "\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "\n",
    "\n",
    "ENV_NAME = generate_random_string()\n",
    "\n",
    "@register_env(ENV_NAME, max_episode_steps=200000)\n",
    "class OurEnv(BaseEnv):\n",
    "    SUPPORTED_REWARD_MODES = [\"none\"]\n",
    "    \"\"\"\n",
    "    This is just a very smart environment for goida transformation from ss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", **kwargs):\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at([1.25, -1.25, 1.5], [0.0, 0.0, 0.2])\n",
    "        return [CameraConfig(\"base_camera\", pose, 256, 256, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        pose = sapien_utils.look_at([8, 8, 1], [10.5, 10.0, 0.0])\n",
    "        return CameraConfig(\n",
    "            \"render_camera\", pose=pose, width=512, height=512, fov=1, near=0.01, far=100\n",
    "        )\n",
    "\n",
    "    def _load_agent(self, options: dict):\n",
    "        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))\n",
    "\n",
    "    def _shift(self, p, shift):\n",
    "        new_p = [0] * len(p)\n",
    "        for i in range(len(p)):\n",
    "            new_p[i] = p[i] + shift[i]\n",
    "        return new_p\n",
    "\n",
    "    def _load_shopping_cart(self, options: dict):\n",
    "        # recommended to use shift = (0,0.5,0)\n",
    "        print(self.unwrapped.agent.robot.get_pose())\n",
    "        if not hasattr(self, 'shopping_cart'):\n",
    "            shopping_cart_asset = os.path.join(assets_dir, \"shoppingCart.glb\")\n",
    "            if not os.path.exists(shopping_cart_asset):\n",
    "                print(f\"Shopping cart asset not found: {shopping_cart_asset}\")\n",
    "            else:\n",
    "                builder = self.scene.create_actor_builder()\n",
    "                builder.add_visual_from_file(filename=shopping_cart_asset, scale=np.array([1.0, 1.0, 1.0]))\n",
    "                builder.add_nonconvex_collision_from_file(filename=shopping_cart_asset, scale=np.array([1.0, 1.0, 1.0]))\n",
    "                shopping_cart_pose = sapien.Pose(p=[11.0, 10.0, 0.0], q=np.array([1, 0, 0, 0]))\n",
    "                builder.set_initial_pose(shopping_cart_pose)\n",
    "                self.shopping_cart = builder.build_static(name=\"shopping_cart\")\n",
    "                self.actors.append(self.shopping_cart)\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.ground = build_ground(self.scene)\n",
    "        self.ground.set_collision_group_bit(group=2, bit_idx=30, bit=1)\n",
    "        self._load_scene_from_json(options)\n",
    "\n",
    "        # self._load_shopping_cart(options)\n",
    "\n",
    "    def _add_noise(self, p, max_noise = 1e-4):\n",
    "        new_p = [0] * len(p)\n",
    "        for i in range(len(p)):\n",
    "            new_p[i] = p[i] + random.randrange(-max_noise, max_noise)\n",
    "        return new_p\n",
    "            \n",
    "\n",
    "\n",
    "    def _process_string(self, s):\n",
    "        if '_' in s:\n",
    "            return s.split('_',1)[0] + '.obj'\n",
    "        if '.' in s:\n",
    "            return s.split('.',1)[0] + '.obj'\n",
    "        return s + '.obj'\n",
    "\n",
    "\n",
    "    def _temp_process_string(self, s):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in \"_.\" or char.isdigit():\n",
    "                return s[:i] + \".obj\"\n",
    "        return s + \".obj\"\n",
    "\n",
    "    def _get_absolute_matrix(self, node, nodes_dict):\n",
    "        current_matrix = np.array(node[2][\"matrix\"])\n",
    "        parent_name = node[0]\n",
    "        while parent_name != \"world\":\n",
    "            # print(f\"Doing GOIDA IN PROCESS for name {node[1]} with parent {node[0]}\")\n",
    "            parent_node = nodes_dict[parent_name]\n",
    "            parent_matrix = np.array(parent_node[2][\"matrix\"])\n",
    "            current_matrix = parent_matrix @ current_matrix\n",
    "            parent_name = parent_node[0]\n",
    "        return current_matrix\n",
    "    \n",
    "    def _get_pq(self, matrix, origin):\n",
    "        matrix = np.array(matrix)\n",
    "        q = quaternions.mat2quat(matrix[:3,:3])\n",
    "        p = matrix[:-1, 3] - origin\n",
    "        return p, q\n",
    "    \n",
    "    def _load_scene_from_json(self, options: dict):\n",
    "        super()._load_scene(options)\n",
    "        self.actors = []\n",
    "\n",
    "        scale = np.array(options.get(\"scale\", [1.0, 1.0, 1.0]))\n",
    "        origin = np.array(options.get(\"origin\", [0.0, 1.0, 0.0]))\n",
    "\n",
    "        with open(json_file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        nodes_dict = {}\n",
    "        for node in data[\"graph\"]:\n",
    "            nodes_dict[node[1]] = node\n",
    "\n",
    "        asset_mapping = {}\n",
    "        if mapping_file is not None:\n",
    "            with open(mapping_file, \"r\") as f:\n",
    "                asset_mapping = json.load(f)\n",
    "\n",
    "        for node in data[\"graph\"]:\n",
    "            parent_name, obj_name, props = node\n",
    "            if ('/' not in obj_name):\n",
    "                abs_matrix = self._get_absolute_matrix(node, nodes_dict)\n",
    "\n",
    "                p, q = self._get_pq(abs_matrix, origin)\n",
    "\n",
    "                obj_name_to_check = self._temp_process_string(obj_name)[:-4]\n",
    "\n",
    "                if obj_name_to_check in asset_mapping:\n",
    "                    asset_file = os.path.join(assets_dir, asset_mapping[obj_name_to_check])\n",
    "                else:\n",
    "                    asset_file = \"\"\n",
    "\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    asset_file = os.path.join(assets_dir, self._temp_process_string(obj_name))\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    asset_file = os.path.splitext(asset_file)[0] + \".glb\"\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    print(\"Not found file for \" + obj_name + \" =(\" + \" ( \" + asset_file + \" )\")\n",
    "                else:\n",
    "                    # print(\"Found file for \" + obj_name + \" =)\" + \" ( \" + asset_file + \" )\")\n",
    "                    builder = self.scene.create_actor_builder()\n",
    "                    builder.add_visual_from_file(filename=asset_file, scale=scale)\n",
    "                    builder.set_initial_pose(sapien.Pose(p=p, q=q))\n",
    "\n",
    "\n",
    "\n",
    "                    if obj_name.startswith('shelf'):\n",
    "                        builder.add_nonconvex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                        actor = builder.build_static(name=obj_name)\n",
    "                    else:\n",
    "                        builder.add_convex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                        actor = builder.build(name=obj_name)\n",
    "\n",
    "                    self.actors.append(actor)\n",
    "\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        if self.robot_uids == \"fetch\":\n",
    "            qpos = np.array(\n",
    "                [\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0.386,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    -np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 3,\n",
    "                    0,\n",
    "                    0.015,\n",
    "                    0.015,\n",
    "                ]\n",
    "            )\n",
    "            self.agent.reset(qpos)\n",
    "            self.agent.robot.set_pose(sapien.Pose([10.0, 10, 0.0]))\n",
    "            self._load_shopping_cart(options)\n",
    "\n",
    "\n",
    "            \n",
    "            self.ground.set_collision_group_bit(\n",
    "                group=2, bit_idx=FETCH_WHEELS_COLLISION_BIT, bit=1\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        return dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(ENV_NAME, robot_uids='fetch', num_envs=1, render_mode=\"rgb_array\", enable_shadow=True)\n",
    "\n",
    "print(env.unwrapped.agent.robot.get_pose())\n",
    "\n",
    "env = RecordEpisode(\n",
    "    env,\n",
    "    \"./videos\", # the directory to save replay videos and trajectories to\n",
    "    # on GPU sim we record intervals, not by single episodes as there are multiple envs\n",
    "    # each 100 steps a new video is saved\n",
    "    max_steps_per_video=100\n",
    ")\n",
    "\n",
    "print(env.unwrapped.agent.robot.get_pose())\n",
    "\n",
    "# step through the environment with random actions\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(env.unwrapped.agent.robot.get_pose())\n",
    "\n",
    "viewer = env.render()\n",
    "if isinstance(viewer, sapien.utils.Viewer):\n",
    "    viewer.paused = False\n",
    "env.render()\n",
    "\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(torch.zeros_like(torch.from_numpy(action)))\n",
    "\n",
    "    env.render()\n",
    "    # env.render_human() # will render with a window if possible\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk_eXp_rlH0O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
