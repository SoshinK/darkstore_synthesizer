{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XG10XGnywZq",
    "outputId": "cbfd5625-76dd-4dd1-c70e-f2f45b15e008"
   },
   "outputs": [],
   "source": [
    "# setup vulkan\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json\n",
    "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
    "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
    "!apt-get install -y --no-install-recommends libvulkan-dev\n",
    "# dependencies\n",
    "!pip install --upgrade mani_skill tyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42fi1mAqzo9a",
    "outputId": "10fb5545-1d2a-4817-a19b-3f52dca30bea"
   },
   "outputs": [],
   "source": [
    "!python -m mani_skill.utils.download_asset RoboCasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK4aQ86s4op8",
    "outputId": "ac22edcc-11b3-4e48-b70a-da494d45a560"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/SoshinK/darkstore_synthesizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "H6V9MvBh57tn",
    "outputId": "62b7dd1c-e762-417a-c7ce-dccd1ddb3a65"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./videos_4_3_style0/0.mp4\", embed=True, width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C493SKRVy0aN",
    "outputId": "2218ceaa-b111-4ff6-d3e5-869e449da883"
   },
   "outputs": [],
   "source": [
    "from mani_skill.envs.tasks.empty_env import EmptyEnv\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from mani_skill.agents.robots.fetch import FETCH_WHEELS_COLLISION_BIT\n",
    "from mani_skill.utils.building.ground import build_ground\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils import common, sapien_utils\n",
    "import sapien\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import mani_skill.envs\n",
    "from tqdm import tqdm\n",
    "from mani_skill.utils.wrappers import RecordEpisode\n",
    "from transforms3d import quaternions\n",
    "import random\n",
    "import string\n",
    "from robocasa_scene_builder import EmptyRoomFromRobocasa\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description=\"Использование: python script.py <путь_к_JSON_файлу> <путь_к_assets> <style id (0-11)> [mapping_file]\"\n",
    "# )\n",
    "# parser.add_argument(\"json_file_path\", help=\"Путь к JSON файлу\")\n",
    "# parser.add_argument(\"assets_dir\", help=\"Путь к assets\")\n",
    "# parser.add_argument(\"style_id\", type=int, help=\"Style id (0-11)\")\n",
    "# parser.add_argument(\"mapping_file\", nargs=\"?\", default=None, help=\"Путь к mapping_file (опционально)\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# json_file_path = args.json_file_path\n",
    "# assets_dir = args.assets_dir\n",
    "# style_id = args.style_id\n",
    "# mapping_file = args.mapping_file\n",
    "\n",
    "\n",
    "json_file_path = \"myscene_4_3.json\"\n",
    "assets_dir = \"darkstore_synthesizer/models\"\n",
    "style_id = 0\n",
    "mapping_file = None\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "\n",
    "\n",
    "ENV_NAME = 'DarkstoreEnv'\n",
    "\n",
    "\n",
    "CELL_SIZE = 1.55\n",
    "\n",
    "def get_arena_data(x_cells=4, y_cells=5, height = 3):\n",
    "    x_size = x_cells * CELL_SIZE\n",
    "    y_size = y_cells * CELL_SIZE\n",
    "    return {\n",
    "        'meta': {\n",
    "            'x_size': x_size,\n",
    "            'y_size': y_size,\n",
    "            'height': height\n",
    "        },\n",
    "        'arena_config': {\n",
    "            'room': {\n",
    "                'walls': [\n",
    "                    {'name': 'wall', 'type': 'wall', 'size': [x_size / 2, height / 2, 0.02], 'pos': [x_size / 2, y_size, height / 2]},\n",
    "                    {'name': 'wall_backing', 'type': 'wall', 'backing': True, 'backing_extended': [True, False], 'size': [x_size / 2, height / 2, 0.1], 'pos': [x_size / 2, y_size, height / 2]},\n",
    "\n",
    "                    {'name': 'wall_front', 'type': 'wall', 'wall_side' : 'front', 'size': [x_size / 2, height / 2, 0.02], 'pos': [x_size / 2, 0, height / 2]},\n",
    "                    {'name': 'wall_front_backing', 'type': 'wall', 'wall_side' : 'front', 'backing': True, 'size': [x_size / 2, height / 2, 0.1], 'pos': [x_size / 2, 0, height / 2]},\n",
    "\n",
    "                    {'name': 'wall_left', 'type': 'wall', 'wall_side': 'left', 'size': [y_size / 2, height / 2, 0.02], 'pos': [0, y_size / 2, height / 2]},\n",
    "                    {'name': 'wall_left_backing', 'type': 'wall', 'wall_side': 'left', 'backing': True, 'size': [y_size / 2, height / 2, 0.1], 'pos': [0, y_size / 2, height / 2]},\n",
    "\n",
    "                    {'name': 'wall_right', 'type': 'wall', 'wall_side': 'right', 'size': [y_size / 2, height / 2, 0.02], 'pos': [x_size, y_size / 2, height / 2]},\n",
    "                    {'name': 'wall_right_backing', 'type': 'wall', 'wall_side': 'right', 'backing': True, 'size': [y_size / 2, height / 2, 0.1], 'pos': [x_size, y_size / 2, height / 2]}\n",
    "                ],\n",
    "                'floor': [\n",
    "                    {'name': 'floor', 'type': 'floor', 'size': [x_size / 2, y_size / 2, 0.02], 'pos': [x_size / 2, y_size / 2, 0.0]},\n",
    "                    {'name': 'floor_backing', 'type': 'floor', 'backing': True, 'size': [x_size / 2, y_size / 2, 0.1], 'pos': [x_size / 2, y_size / 2, 0.0]}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "@register_env(ENV_NAME, max_episode_steps=200000)\n",
    "class DarkstoreEnv(BaseEnv):\n",
    "    SUPPORTED_REWARD_MODES = [\"none\"]\n",
    "    \"\"\"\n",
    "    This is just a very smart environment for goida transformation from ss\n",
    "    \"\"\"\n",
    "    IMPORTED_SS_SCENE_SHIFT = np.array([CELL_SIZE / 2, CELL_SIZE / 2, 0])\n",
    "\n",
    "    def __init__(self, *args, robot_uids=\"panda\", arena_config = None, meta = None, style_ids = 0, **kwargs):\n",
    "        self.style_ids = style_ids\n",
    "        self.arena_config = arena_config\n",
    "        self.x_size = meta['x_size']\n",
    "        self.y_size = meta['y_size']\n",
    "        self.height = meta['height']\n",
    "        super().__init__(*args, robot_uids=robot_uids, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _default_sensor_configs(self):\n",
    "        pose = sapien_utils.look_at([1.25, -1.25, 1.5], [0.0, 0.0, 0.2])\n",
    "        return [CameraConfig(\"base_camera\", pose, 256, 256, np.pi / 2, 0.01, 100)]\n",
    "\n",
    "    @property\n",
    "    def _default_human_render_camera_configs(self):\n",
    "        # pose = sapien_utils.look_at([0.2, 0.2, 4], [5, 5, 2])\n",
    "        pose = sapien_utils.look_at([self.x_size - 0.1, self.y_size - 0.1, self.height], [0, 0, 0])\n",
    "        return CameraConfig(\n",
    "            \"render_camera\", pose=pose, width=512, height=512, fov=1, near=0.01, far=100\n",
    "        )\n",
    "\n",
    "    def _load_agent(self, options: dict):\n",
    "        super()._load_agent(options, sapien.Pose(p=[-0.615, 0, 0]))\n",
    "\n",
    "    def _load_scene(self, options: dict):\n",
    "        self.scene_builder = EmptyRoomFromRobocasa(self, arena_config=self.arena_config)\n",
    "        self.scene_builder.build(self.style_ids)\n",
    "        self._load_scene_from_json(options)\n",
    "\n",
    "\n",
    "    def _process_string(self, s):\n",
    "        if '_' in s:\n",
    "            return s.split('_',1)[0] + '.obj'\n",
    "        if '.' in s:\n",
    "            return s.split('.',1)[0] + '.obj'\n",
    "        return s + '.obj'\n",
    "\n",
    "    def _temp_process_string(self, s):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in \"_.\" or char.isdigit():\n",
    "                return s[:i] + \".obj\"\n",
    "        return s + \".obj\"\n",
    "\n",
    "    def _get_absolute_matrix(self, node, nodes_dict):\n",
    "        current_matrix = np.array(node[2][\"matrix\"])\n",
    "        parent_name = node[0]\n",
    "        while parent_name != \"world\":\n",
    "            # print(f\"Doing GOIDA IN PROCESS for name {node[1]} with parent {node[0]}\")\n",
    "            parent_node = nodes_dict[parent_name]\n",
    "            parent_matrix = np.array(parent_node[2][\"matrix\"])\n",
    "            current_matrix = parent_matrix @ current_matrix\n",
    "            parent_name = parent_node[0]\n",
    "        return current_matrix\n",
    "\n",
    "    def _get_pq(self, matrix, origin):\n",
    "        matrix = np.array(matrix)\n",
    "        q = quaternions.mat2quat(matrix[:3,:3])\n",
    "        p = matrix[:-1, 3] - origin\n",
    "        return p, q\n",
    "\n",
    "    def _load_scene_from_json(self, options: dict):\n",
    "        super()._load_scene(options)\n",
    "        self.actors = []\n",
    "\n",
    "        scale = np.array(options.get(\"scale\", [1.0, 1.0, 1.0]))\n",
    "        origin = np.array(options.get(\"origin\", [0.0, 1.0, 0.0]))\n",
    "\n",
    "        with open(json_file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        nodes_dict = {}\n",
    "        for node in data[\"graph\"]:\n",
    "            nodes_dict[node[1]] = node\n",
    "\n",
    "        asset_mapping = {}\n",
    "        if mapping_file is not None:\n",
    "            with open(mapping_file, \"r\") as f:\n",
    "                asset_mapping = json.load(f)\n",
    "\n",
    "        for node in data[\"graph\"]:\n",
    "            parent_name, obj_name, props = node\n",
    "            if ('/' not in obj_name):\n",
    "                abs_matrix = self._get_absolute_matrix(node, nodes_dict)\n",
    "\n",
    "                p, q = self._get_pq(abs_matrix, origin)\n",
    "\n",
    "                obj_name_to_check = self._temp_process_string(obj_name)[:-4]\n",
    "\n",
    "                if obj_name_to_check in asset_mapping:\n",
    "                    asset_file = os.path.join(assets_dir, asset_mapping[obj_name_to_check])\n",
    "                else:\n",
    "                    asset_file = \"\"\n",
    "\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    asset_file = os.path.join(assets_dir, self._temp_process_string(obj_name))\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    asset_file = os.path.splitext(asset_file)[0] + \".glb\"\n",
    "\n",
    "                if not os.path.exists(asset_file):\n",
    "                    print(\"Not found file for \" + obj_name + \" =(\" + \" ( \" + asset_file + \" )\")\n",
    "                else:\n",
    "                    # print(\"Found file for \" + obj_name + \" =)\" + \" ( \" + asset_file + \" )\")\n",
    "                    builder = self.scene.create_actor_builder()\n",
    "                    builder.add_visual_from_file(filename=asset_file, scale=scale)\n",
    "                    builder.set_initial_pose(sapien.Pose(p=p, q=q))\n",
    "\n",
    "\n",
    "\n",
    "                    if obj_name.startswith('shelf'):\n",
    "                        builder.add_nonconvex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                        actor = builder.build_static(name=obj_name)\n",
    "                    else:\n",
    "                        builder.add_convex_collision_from_file(filename=asset_file, scale=scale)\n",
    "                        actor = builder.build(name=obj_name)\n",
    "\n",
    "                    self.actors.append(actor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_episode(self, env_idx: torch.Tensor, options: dict):\n",
    "        if self.robot_uids == \"fetch\":\n",
    "            qpos = np.array(\n",
    "                [\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0.386,\n",
    "                    0,\n",
    "                    0,\n",
    "                    0,\n",
    "                    -np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 4,\n",
    "                    0,\n",
    "                    np.pi / 3,\n",
    "                    0,\n",
    "                    0.015,\n",
    "                    0.015,\n",
    "                ]\n",
    "            )\n",
    "            self.agent.reset(qpos)\n",
    "            self.agent.robot.set_pose(sapien.Pose([0.5, 0.5, 0.0]))\n",
    "\n",
    "            # self.ground.set_collision_group_bit(\n",
    "            #     group=2, bit_idx=FETCH_WHEELS_COLLISION_BIT, bit=1\n",
    "            # )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        return {}\n",
    "\n",
    "    def _get_obs_extra(self, info: Dict):\n",
    "        return dict()\n",
    "\n",
    "with open(json_file_path, \"r\") as f: # big_scene , one_shelf_many_milk_scene , customize\n",
    "    data = json.load(f)\n",
    "\n",
    "n = data['meta']['n']\n",
    "m = data['meta']['m']\n",
    "arena_data = get_arena_data(x_cells=n, y_cells=m, height=4)\n",
    "\n",
    "env = gym.make(ENV_NAME, robot_uids='fetch', style_ids = [style_id], num_envs=1, render_mode=\"rgb_array\", enable_shadow=True, **arena_data)\n",
    "\n",
    "\n",
    "env = RecordEpisode(\n",
    "    env,\n",
    "    f\"./videos_{n}_{m}_style{style_id}\", # the directory to save replay videos and trajectories to\n",
    "    # on GPU sim we record intervals, not by single episodes as there are multiple envs\n",
    "    # each 100 steps a new video is saved\n",
    "    max_steps_per_video=100\n",
    ")\n",
    "\n",
    "# step through the environment with random actions\n",
    "obs, _ = env.reset()\n",
    "\n",
    "\n",
    "viewer = env.render()\n",
    "if isinstance(viewer, sapien.utils.Viewer):\n",
    "    viewer.paused = False\n",
    "env.render()\n",
    "\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(torch.zeros_like(torch.from_numpy(action)))\n",
    "\n",
    "    env.render()\n",
    "    # env.render_human() # will render with a window if possible\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUIzZts9zRZk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sapien\n",
    "\n",
    "from mani_skill.envs.tasks import PickCubeEnv\n",
    "from mani_skill.examples.motionplanning.panda.motionplanner import \\\n",
    "    PandaArmMotionPlanningSolver\n",
    "from mani_skill.examples.motionplanning.panda.utils import (\n",
    "    compute_grasp_info_by_obb, get_actor_obb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def solve_by_coords(env: PickCubeEnv, target: Actor, goal_pose: sapien.Pose, vis=False):\n",
    "    planner = PandaArmMotionPlanningSolver(\n",
    "        env,\n",
    "        debug=debug,\n",
    "        vis=vis,\n",
    "        base_pose=env.unwrapped.agent.robot.pose,\n",
    "        visualize_target_grasp_pose=vis,\n",
    "        print_env_info=False,\n",
    "    )\n",
    "\n",
    "    FINGER_LENGTH = 0.025\n",
    "    env = env.unwrapped\n",
    "\n",
    "    # retrieves the object oriented bounding box (trimesh box object)\n",
    "    obb = get_actor_obb(target)\n",
    "\n",
    "    approaching = np.array([0, 0, -1])\n",
    "    # get transformation matrix of the tcp pose, is default batched and on torch\n",
    "    target_closing = env.agent.tcp.pose.to_transformation_matrix()[0, :3, 1].cpu().numpy()\n",
    "    # we can build a simple grasp pose using this information for Panda\n",
    "    grasp_info = compute_grasp_info_by_obb(\n",
    "        obb,\n",
    "        approaching=approaching,\n",
    "        target_closing=target_closing,\n",
    "        depth=FINGER_LENGTH,\n",
    "    )\n",
    "    closing, center = grasp_info[\"closing\"], grasp_info[\"center\"]\n",
    "    grasp_pose = env.agent.build_grasp_pose(approaching, closing, target.pose.sp.p)\n",
    "    agent_z_reach_pose = env.agent.get_pose()\n",
    "    agent_z_reach_pose[2] = grasp_pose[2]\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Reach\n",
    "    # -------------------------------------------------------------------------- #\n",
    "\n",
    "    # reach_pose = grasp_pose * sapien.Pose([0, 0, -0.05])\n",
    "    planner.move_to_pose_with_screw(agent_z_reach_pose)\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Grasp\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    planner.move_to_pose_with_screw(grasp_pose)\n",
    "\n",
    "    # reach_pose = grasp_pose * sapien.Pose([0, 0, -0.05])\n",
    "    planner.close_gripper()\n",
    "\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    # Move to goal pose\n",
    "    # -------------------------------------------------------------------------- #\n",
    "    grasp_pose[:2] = goal_pose[:2]\n",
    "    res = planner.move_to_pose_with_screw(grasp_pose)\n",
    "    res = planner.move_to_pose_with_screw(goal_pose)\n",
    "\n",
    "    planner.close()\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
